{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOdLg5jzMkGpNcyzw/B0HmS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Part 2: Model Deployment"],"metadata":{"id":"3Y_sssdjOxWL"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ps1bpZ7UOKUO","executionInfo":{"status":"ok","timestamp":1766980501398,"user_tz":-480,"elapsed":11636,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"outputs":[],"source":["'''\n","Since we have done eda and some preprocessing in eda notebook, we now moving forward to model training. we have also loaded previously\n","used of everything into here to continue, and also added some imports to use for model training.\n","\n","'''\n","\n","# PyTorch\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn # for loading model, model training\n","import torch.optim as optim # model training\n","\n","# Torchvision\n","from torchvision.datasets import Flowers102\n","from torchvision import models\n","\n","# Albumentations\n","import albumentations as aug\n","from albumentations.pytorch import ToTensorV2\n","\n","# Others\n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","source":["from google.colab import drive # mount drive first to access google drive\n","drive.mount('/content/drive')\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXs7TV4iPscw","executionInfo":{"status":"ok","timestamp":1766982177972,"user_tz":-480,"elapsed":56074,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}},"outputId":"234e6d9d-031e-4d67-a4ee-70af302c0f68"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["'''\n","we now save each split of the dataset into drive, that way we dont have to download everytime, we can just load from drive\n","\n","'''\n","\n","from torchvision.datasets import Flowers102\n","import torch\n","import os\n","\n","\n","save_dir = '/content/drive/MyDrive/flowers102'\n","os.makedirs(save_dir, exist_ok=True)  # ensure folder exists\n","\n","splits = ['train', 'val', 'test']\n","\n","for split in splits:\n","    ds = Flowers102(root='content', split=split, download=True)\n","    torch.save(ds, os.path.join(save_dir, f'{split}_dataset.pt'))  # save each split with a filename\n"],"metadata":{"id":"ICaVfZ-bUz6y","executionInfo":{"status":"ok","timestamp":1766983521631,"user_tz":-480,"elapsed":144,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["From here to before model training, we load all the albumentations, dataloader and preprocess etc. from previous notebook to continue our model training, all here are the same from previous."],"metadata":{"id":"Ne025cksWkDW"}},{"cell_type":"code","source":["train_augment = aug.Compose([ #we only augment training set, this way the model will see different varieties, to learn better.\n","    aug.Resize(224, 224), #resize\n","    aug.HorizontalFlip(p=0.5), #horizontal flip\n","    aug.Rotate(limit=15, p=0.5),#rotation\n","    aug.RandomBrightnessContrast(p=0.3),#adjust brightness and contrast\n","    aug.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), #normalize, values are standard\n","    ToTensorV2() # convert numpy array to tensor\n","])"],"metadata":{"id":"WW_p3IVaXlv0","executionInfo":{"status":"ok","timestamp":1766983526178,"user_tz":-480,"elapsed":24,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Validation and test set no transfomrs, to keep it as original as it is.\n","\"\"\"\n","\n","val_test_noaug = aug.Compose([\n","    aug.Resize(224, 224), #resize\n","    aug.Normalize(\n","        mean=(0.485, 0.456, 0.406),  #normalize, values are standard\n","        std=(0.229, 0.224, 0.225)\n","    ),\n","    ToTensorV2()# convert numpy array to tensor\n","])\n"],"metadata":{"id":"lul2JKScXl1r","executionInfo":{"status":"ok","timestamp":1766983528690,"user_tz":-480,"elapsed":19,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","\n","class Albumentations(Dataset):\n","    \"\"\"\n","    Wrapper to apply Albumentations transforms to a torchvision dataset.\n","    \"\"\"\n","\n","    def __init__(self, dataset, transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, label = self.dataset[idx]\n","        image = np.array(image)  # Convert PIL to NumPy\n","        if self.transform:\n","            image = self.transform(image=image)[\"image\"]\n","        return image, label\n"],"metadata":{"id":"WYstwUk2OwVv","executionInfo":{"status":"ok","timestamp":1766983531362,"user_tz":-480,"elapsed":28,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["'''\n","Load the dataset from drive\n","Same from previous notebook eda\n","\n","'''\n","\n","save_dir = '/content/drive/MyDrive/flowers102'\n","\n","with torch.serialization.safe_globals([Flowers102]):\n","    train = torch.load(f'{save_dir}/train_dataset.pt', weights_only=False) # load each split\n","    val   = torch.load(f'{save_dir}/val_dataset.pt', weights_only=False) # weight_only = False , allows loading full python objects, like custom class Flowers102.\n","    test  = torch.load(f'{save_dir}/test_dataset.pt', weights_only=False)\n","# Weight_only = False\n","# unpickling arbitrary code can run malicious code if file not trushworthy.\n","# Required here because the .pt file contains a custom Dataset object, not just raw tensors.\n","\n","train_data = Albumentations(train, train_augment)\n","val_data   = Albumentations(val, val_test_noaug)\n","test_data  = Albumentations(test, val_test_noaug)\n","\n"],"metadata":{"id":"9ZdOtyHBXl5h","executionInfo":{"status":"ok","timestamp":1766983861691,"user_tz":-480,"elapsed":60,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Create PyTorch DataLoaders for training, validation, and testing.\n","\"\"\"\n","\n","train_loader = DataLoader(\n","    train_data,\n","    batch_size=16, # number of samples processed together before model update its parameters, 16 is good as too big will require more memory\n","    shuffle=True,        # order randomised\n","    num_workers=2  # numbers of parallel processes to load batches\n",")\n","\n","val_loader = DataLoader(\n","    val_data,\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=2\n",")\n","\n","test_loader = DataLoader(\n","    test_data,\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=2\n",")\n"],"metadata":{"id":"Bcdn2kFuXl8p","executionInfo":{"status":"ok","timestamp":1766983978207,"user_tz":-480,"elapsed":24,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Up until here from the previous notebook, now moving forward to model deployment"],"metadata":{"id":"-OOqRAnte6B0"}},{"cell_type":"code","source":["import random\n","\n","# Reproducibility function\n","\n","def set_seed(seed=42):\n","    \"\"\"\n","    Set random seeds for reproducibility.\n","\n","    Args:\n","        seed (int): seed value to set for torch, numpy, random.\n","    \"\"\"\n","    # seeds are used for ensuring are consistent everytime we train, useful comparing result since we have\n","    # done those preprocessing earlier. without seeds, might get different training results\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # Ensure deterministic behavior in CuDNN\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed()"],"metadata":{"id":"ANoMwHlmXl_V","executionInfo":{"status":"ok","timestamp":1766985887324,"user_tz":-480,"elapsed":32,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\n","# Model setup\n","\n","# Load pre-trained ResNet50 model\n","# Reason on choosing: deep enough to capture complex flower features, pretrained on ImageNet, low level features are already well learned, easy to fine-tune\n","model = models.resnet50(pretrained=True)"],"metadata":{"id":"2esWtoKeXmDv","executionInfo":{"status":"ok","timestamp":1766985902361,"user_tz":-480,"elapsed":514,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Freeze early layers to retain learned features; fine-tune only last block and classifier, to reduce time and overfitting\n","# Freeze to learn the complex features\n","for name, param in model.named_parameters():\n","    if \"layer4\" not in name:  # layer4 is last convolutional block\n","        param.requires_grad = False\n","\n","# Replace classifier for 102 flower classes\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, 102)"],"metadata":{"id":"aKchdsiDhIhg","executionInfo":{"status":"ok","timestamp":1766985906954,"user_tz":-480,"elapsed":35,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device) # if have gpu then use gpu else use cpu\n","\n","\n","\n","# Loss function with class weighting\n","\n","# compute class weights to handle class imbalances\n","labels_list = [label for _, label in train_data]  # collect all training labels\n","class_counts = Counter(labels_list)\n","total_count = sum(class_counts.values())# count occurence for each class\n","# Inverse frequency weighting\n","class_weights = [total_count / class_counts[i] for i in range(102)]\n","class_weights = torch.tensor(class_weights).float().to(device)\n","# uses crossentropyloss to handle class imbalances, since we have already identified class imbalance exist,\n","# give higher weights to less sample, lower weights to more samples\n","criterion = nn.CrossEntropyLoss(weight=class_weights)"],"metadata":{"id":"CuATfGJShIqS","executionInfo":{"status":"ok","timestamp":1766986523735,"user_tz":-480,"elapsed":5633,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["\n","# Optimizer & scheduler\n","\n","optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)# adam is most common and typically used for updating model, update non freeze layer\n","# parameters using gradients from backpropagation\n","# regularize weights and improve generalization\n","# reduce learning rate if validation loss plateaus\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"],"metadata":{"id":"7ow-NJuyhIuX","executionInfo":{"status":"ok","timestamp":1766986808104,"user_tz":-480,"elapsed":43,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm # for seeing real time progress\n","# Training function\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15, patience=3, save_path='best_model.pt'):\n","    \"\"\"\n","    Train a model with early stopping and checkpointing.\n","\n","    Args:\n","        model: PyTorch model to train\n","        train_loader, val_loader: DataLoaders for training and validation\n","        criterion: loss function\n","        optimizer: optimizer\n","        scheduler: learning rate scheduler\n","        num_epochs: maximum number of epochs\n","        patience: early stopping patience (in epochs)\n","        save_path: path to save the best model\n","    \"\"\"\n","    best_val_loss = float('inf')  # initialize best validation loss\n","    epochs_no_improve = 0  # counter for early stopping\n","\n","    for epoch in range(num_epochs):\n","\n","        # Training loop\n","        model.train()  # set model to training mode\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()  # clear gradients\n","            outputs = model(images)  # forward pass\n","            loss = criterion(outputs, labels)  # compute loss\n","            loss.backward()  # backpropagation\n","            optimizer.step()  # update weights\n","\n","            train_loss += loss.item() * images.size(0)  # accumulate batch loss\n","            _, preds = torch.max(outputs, 1)  # get predicted class\n","            correct += (preds == labels).sum().item()  # count correct predictions\n","            total += labels.size(0)  # accumulate total samples\n","\n","        train_loss /= total  # average training loss\n","        train_acc = correct / total  # training accuracy\n","\n","\n","        # Validation loop\n","\n","        model.eval()  # set model to evaluation mode\n","        val_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():  # disable gradient computation\n","            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * images.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                correct += (preds == labels).sum().item()\n","                total += labels.size(0)\n","\n","        val_loss /= total  # average validation loss\n","        val_acc = correct / total  # validation accuracy\n","\n","        print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f} Accuracy {train_acc:.4f} | Val Loss {val_loss:.4f} Accuracy {val_acc:.4f}\")\n","\n","        # Scheduler step\n","        scheduler.step(val_loss)  # adjust LR if validation loss plateaus\n","\n","        # Early stopping & checkpoint\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), save_path)  # save best model\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve >= patience:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","                break"],"metadata":{"id":"_89Mh4Zvnwj9","executionInfo":{"status":"ok","timestamp":1766988010934,"user_tz":-480,"elapsed":22,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# model training\n","\n","train_model(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    num_epochs=15,\n","    patience=3,\n","    save_path='/content/drive/MyDrive/flowers102_resnet50_best.pt'\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"mqymV7llnwsX","executionInfo":{"status":"error","timestamp":1766996895066,"user_tz":-480,"elapsed":110725,"user":{"displayName":"Tan Wee Kang","userId":"12751589017177132703"}},"outputId":"51686b10-1419-477f-add2-a9d5eade7ff5"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/15 - Train:  28%|██▊       | 18/64 [01:50<04:42,  6.14s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2294265960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-311933366.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, save_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}